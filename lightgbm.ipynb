{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines.utils import concordance_index\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's binary_logloss: 0.562588\tvalid_1's binary_logloss: 0.584943\n",
      "\n",
      "Training Accuracy: 0.7106\n",
      "Validation Accuracy: 0.6910\n",
      "Test Accuracy: 0.6810\n",
      "\n",
      "Test Set C-index: 0.3474\n",
      "\n",
      "Top 10 Features:\n",
      "                   feature  importance\n",
      "40              age_at_hct         229\n",
      "29                year_hct         185\n",
      "36               donor_age         174\n",
      "42              gvhd_proph         155\n",
      "0                       ID         155\n",
      "14        prim_disease_hct         151\n",
      "48         karnofsky_score         139\n",
      "47       comorbidity_score         132\n",
      "44               sex_match         125\n",
      "27  conditioning_intensity         116\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df):\n",
    "    data = df.copy()\n",
    "    categorical_cols = [\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "        'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate', 'efs'\n",
    "    ]\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].astype(str).fillna('Missing')\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "    numerical_cols = [col for col in data.columns if col not in categorical_cols]\n",
    "    for col in numerical_cols:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].fillna(data[col].median())\n",
    "    return data\n",
    "\n",
    "def train_lightgbm(df, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    assert train_size + val_size + test_size == 1.0, \"Split sizes must sum to 1\"\n",
    "    \n",
    "    # Prepare the data\n",
    "    data = prepare_data(df)\n",
    "    X = data.drop(['efs', 'efs_time'], axis=1)\n",
    "    y = data['efs']  # Event indicator\n",
    "    event_times = data['efs_time']  # Time to event\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp, t_train, t_temp = train_test_split(\n",
    "        X, y, event_times, test_size=(val_size + test_size), random_state=42\n",
    "    )\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test, t_val, t_test = train_test_split(\n",
    "        X_temp, y_temp, t_temp, test_size=(1 - val_proportion), random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X):.2%})\")\n",
    "    print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X):.2%})\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X):.2%})\")\n",
    "    \n",
    "    # Define LightGBM classifier with parameters to prevent overfitting\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        objective='binary',         # Binary classification\n",
    "        metric='binary_logloss',    # Evaluation metric\n",
    "        max_depth=4,                # Limit tree depth\n",
    "        learning_rate=0.05,         # Slow learning rate\n",
    "        n_estimators=200,           # Number of trees\n",
    "        min_child_samples=20,       # Minimum samples per leaf (similar to min_child_weight)\n",
    "        subsample=0.7,              # Fraction of data per tree\n",
    "        colsample_bytree=0.7,       # Fraction of features per tree\n",
    "        reg_alpha=1.0,              # L1 regularization\n",
    "        reg_lambda=2.0,             # L2 regularization\n",
    "        random_state=42,\n",
    "        verbose=-1                  # Suppress warnings\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20)],  # Early stopping\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = lgb_model.score(X_train, y_train)\n",
    "    val_score = lgb_model.score(X_val, y_val)\n",
    "    test_score = lgb_model.score(X_test, y_test)\n",
    "    print(f\"\\nTraining Accuracy: {train_score:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_score:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "    \n",
    "    # Predict probabilities for C-index\n",
    "    y_test_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    le = LabelEncoder().fit(df['efs'])  # Re-fit to get original encoding\n",
    "        # Calculate C-index for the test set using lifelines\n",
    "    c_index = concordance_index(event_times=t_test, \n",
    "                               predicted_scores=y_test_pred_proba, \n",
    "                               event_observed=y_test)\n",
    "    \n",
    "    print(f\"\\nTest Set C-index: {c_index:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 10 Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return lgb_model, X_train, X_val, X_test, y_train, y_val, y_test, t_train, t_val, t_test\n",
    "\n",
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Train the model\n",
    "model, X_train, X_val, X_test, y_train, y_val, y_test, t_train, t_val, t_test = train_lightgbm(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 23040 (80.00%)\n",
      "Validation set size: 2880 (10.00%)\n",
      "Test set size: 2880 (10.00%)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[194]\ttraining's binary_logloss: 0.566034\tvalid_1's binary_logloss: 0.598775\n",
      "\n",
      "Training Accuracy: 0.7077\n",
      "Validation Accuracy: 0.6719\n",
      "Test Accuracy: 0.6969\n",
      "\n",
      "Test Set C-index: 0.3458\n",
      "\n",
      "Top 10 Features:\n",
      "              feature  importance\n",
      "29           year_hct         194\n",
      "40         age_at_hct         178\n",
      "36          donor_age         157\n",
      "47  comorbidity_score         144\n",
      "42         gvhd_proph         143\n",
      "48    karnofsky_score         143\n",
      "14   prim_disease_hct         139\n",
      "0                  ID         137\n",
      "44          sex_match         136\n",
      "16         cmv_status         109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# df = df.drop([\"tce_imm_match\"], axis=1)\n",
    "# df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "# Drop the original prim_disease_hct column\n",
    "# df = df.drop('prim_disease_hct', axis=1)\n",
    "\n",
    "model, X_train, X_val, X_test, y_train, y_val, y_test, t_train, t_val, t_test = train_lightgbm(\n",
    "    df,\n",
    "    train_size=0.8,\n",
    "    val_size=0.10,\n",
    "    test_size=0.10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
