{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "['dri_score_High' 'dri_score_High - TED AML case <missing cytogenetics'\n",
      " 'dri_score_Intermediate'\n",
      " 'dri_score_Intermediate - TED AML case <missing cytogenetics'\n",
      " 'dri_score_Low' 'dri_score_Missing' 'dri_score_Missing disease status'\n",
      " 'dri_score_N/A - disease not classifiable'\n",
      " 'dri_score_N/A - non-malignant indication' 'dri_score_N/A - pediatric'\n",
      " 'dri_score_TBD cytogenetics' 'dri_score_Very high'\n",
      " 'psych_disturb_Missing' 'psych_disturb_No' 'psych_disturb_Not done'\n",
      " 'psych_disturb_Yes' 'cyto_score_Favorable' 'cyto_score_Intermediate'\n",
      " 'cyto_score_Missing' 'cyto_score_Normal' 'cyto_score_Not tested'\n",
      " 'cyto_score_Other' 'cyto_score_Poor' 'cyto_score_TBD' 'diabetes_Missing'\n",
      " 'diabetes_No' 'diabetes_Not done' 'diabetes_Yes' 'tbi_status_No TBI'\n",
      " 'tbi_status_TBI + Cy +- Other'\n",
      " 'tbi_status_TBI +- Other, -cGy, fractionated'\n",
      " 'tbi_status_TBI +- Other, -cGy, single'\n",
      " 'tbi_status_TBI +- Other, -cGy, unknown dose'\n",
      " 'tbi_status_TBI +- Other, <=cGy' 'tbi_status_TBI +- Other, >cGy'\n",
      " 'tbi_status_TBI +- Other, unknown dose' 'arrhythmia_Missing'\n",
      " 'arrhythmia_No' 'arrhythmia_Not done' 'arrhythmia_Yes'\n",
      " 'graft_type_Bone marrow' 'graft_type_Peripheral blood'\n",
      " 'vent_hist_Missing' 'vent_hist_No' 'vent_hist_Yes' 'renal_issue_Missing'\n",
      " 'renal_issue_No' 'renal_issue_Not done' 'renal_issue_Yes'\n",
      " 'pulm_severe_Missing' 'pulm_severe_No' 'pulm_severe_Not done'\n",
      " 'pulm_severe_Yes' 'prim_disease_hct_AI' 'prim_disease_hct_ALL'\n",
      " 'prim_disease_hct_AML' 'prim_disease_hct_CML' 'prim_disease_hct_HD'\n",
      " 'prim_disease_hct_HIS' 'prim_disease_hct_IEA' 'prim_disease_hct_IIS'\n",
      " 'prim_disease_hct_IMD' 'prim_disease_hct_IPA' 'prim_disease_hct_MDS'\n",
      " 'prim_disease_hct_MPN' 'prim_disease_hct_NHL'\n",
      " 'prim_disease_hct_Other acute leukemia' 'prim_disease_hct_Other leukemia'\n",
      " 'prim_disease_hct_PCD' 'prim_disease_hct_SAA'\n",
      " 'prim_disease_hct_Solid tumor' 'cmv_status_+/+' 'cmv_status_+/-'\n",
      " 'cmv_status_-/+' 'cmv_status_-/-' 'cmv_status_Missing'\n",
      " 'tce_imm_match_G/B' 'tce_imm_match_G/G' 'tce_imm_match_H/B'\n",
      " 'tce_imm_match_H/H' 'tce_imm_match_Missing' 'tce_imm_match_P/B'\n",
      " 'tce_imm_match_P/G' 'tce_imm_match_P/H' 'tce_imm_match_P/P'\n",
      " 'rituximab_Missing' 'rituximab_No' 'rituximab_Yes' 'prod_type_BM'\n",
      " 'prod_type_PB' 'cyto_score_detail_Favorable'\n",
      " 'cyto_score_detail_Intermediate' 'cyto_score_detail_Missing'\n",
      " 'cyto_score_detail_Not tested' 'cyto_score_detail_Poor'\n",
      " 'cyto_score_detail_TBD' 'conditioning_intensity_MAC'\n",
      " 'conditioning_intensity_Missing'\n",
      " 'conditioning_intensity_N/A, F(pre-TED) not submitted'\n",
      " 'conditioning_intensity_NMA' 'conditioning_intensity_No drugs reported'\n",
      " 'conditioning_intensity_RIC' 'conditioning_intensity_TBD'\n",
      " 'ethnicity_Hispanic or Latino' 'ethnicity_Missing'\n",
      " 'ethnicity_Non-resident of the U.S.' 'ethnicity_Not Hispanic or Latino'\n",
      " 'obesity_Missing' 'obesity_No' 'obesity_Not done' 'obesity_Yes'\n",
      " 'mrd_hct_Missing' 'mrd_hct_Negative' 'mrd_hct_Positive'\n",
      " 'in_vivo_tcd_Missing' 'in_vivo_tcd_No' 'in_vivo_tcd_Yes'\n",
      " 'tce_match_Fully matched' 'tce_match_GvH non-permissive'\n",
      " 'tce_match_HvG non-permissive' 'tce_match_Missing' 'tce_match_Permissive'\n",
      " 'hepatic_severe_Missing' 'hepatic_severe_No' 'hepatic_severe_Not done'\n",
      " 'hepatic_severe_Yes' 'prior_tumor_Missing' 'prior_tumor_No'\n",
      " 'prior_tumor_Not done' 'prior_tumor_Yes' 'peptic_ulcer_Missing'\n",
      " 'peptic_ulcer_No' 'peptic_ulcer_Not done' 'peptic_ulcer_Yes'\n",
      " 'gvhd_proph_CDselect +- other' 'gvhd_proph_CDselect alone'\n",
      " 'gvhd_proph_CSA + MMF +- others(not FK)'\n",
      " 'gvhd_proph_CSA + MTX +- others(not MMF,FK)'\n",
      " 'gvhd_proph_CSA +- others(not FK,MMF,MTX)' 'gvhd_proph_CSA alone'\n",
      " 'gvhd_proph_Cyclophosphamide +- others'\n",
      " 'gvhd_proph_Cyclophosphamide alone' 'gvhd_proph_FK+ MMF +- others'\n",
      " 'gvhd_proph_FK+ MTX +- others(not MMF)'\n",
      " 'gvhd_proph_FK+- others(not MMF,MTX)' 'gvhd_proph_FKalone'\n",
      " 'gvhd_proph_Missing' 'gvhd_proph_No GvHD Prophylaxis'\n",
      " 'gvhd_proph_Other GVHD Prophylaxis'\n",
      " 'gvhd_proph_Parent Q = yes, but no agent'\n",
      " 'gvhd_proph_TDEPLETION +- other' 'gvhd_proph_TDEPLETION alone'\n",
      " 'rheum_issue_Missing' 'rheum_issue_No' 'rheum_issue_Not done'\n",
      " 'rheum_issue_Yes' 'sex_match_F-F' 'sex_match_F-M' 'sex_match_M-F'\n",
      " 'sex_match_M-M' 'sex_match_Missing'\n",
      " 'race_group_American Indian or Alaska Native' 'race_group_Asian'\n",
      " 'race_group_Black or African-American' 'race_group_More than one race'\n",
      " 'race_group_Native Hawaiian or other Pacific Islander' 'race_group_White'\n",
      " 'hepatic_mild_Missing' 'hepatic_mild_No' 'hepatic_mild_Not done'\n",
      " 'hepatic_mild_Yes' 'tce_div_match_Bi-directional non-permissive'\n",
      " 'tce_div_match_GvH non-permissive' 'tce_div_match_HvG non-permissive'\n",
      " 'tce_div_match_Missing' 'tce_div_match_Permissive mismatched'\n",
      " 'donor_related_Missing' 'donor_related_Multiple donor (non-UCB)'\n",
      " 'donor_related_Related' 'donor_related_Unrelated' 'melphalan_dose_MEL'\n",
      " 'melphalan_dose_Missing' 'melphalan_dose_N/A, Mel not given'\n",
      " 'cardiac_Missing' 'cardiac_No' 'cardiac_Not done' 'cardiac_Yes'\n",
      " 'pulm_moderate_Missing' 'pulm_moderate_No' 'pulm_moderate_Not done'\n",
      " 'pulm_moderate_Yes' 'hla_match_c_high' 'hla_high_res_8' 'hla_low_res_6'\n",
      " 'hla_high_res_6' 'hla_high_res_10' 'hla_match_dqb1_high' 'hla_nmdp_6'\n",
      " 'hla_match_c_low' 'hla_match_drb1_low' 'hla_match_dqb1_low' 'year_hct'\n",
      " 'hla_match_a_high' 'donor_age' 'hla_match_b_low' 'age_at_hct'\n",
      " 'hla_match_a_low' 'hla_match_b_high' 'comorbidity_score'\n",
      " 'karnofsky_score' 'hla_low_res_8' 'hla_match_drb1_high' 'hla_low_res_10']\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 2880 (10.00%)\n",
      "Test set size: 8640 (30.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       80337.3923           1.1510           20.66m\n",
      "         2       80013.1655           1.3291           20.51m\n",
      "         3       80201.7783           1.0289           20.67m\n",
      "         4       79747.0460           1.5409           20.54m\n",
      "         5       79951.1586           0.8490           20.38m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 135\u001b[0m\n\u001b[1;32m    131\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/equity-post-HCT-survival-predictions/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# df['conditioning_intensity'] = df['conditioning_intensity'].fillna(\"TBD\")\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Call the function, specifying the ID column to remove\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test, val_pred, test_pred \u001b[38;5;241m=\u001b[39m \u001b[43msplit_train_and_evaluate_with_sksurv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust this to match your actual ID column name\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 95\u001b[0m, in \u001b[0;36msplit_train_and_evaluate_with_sksurv\u001b[0;34m(df, train_size, val_size, test_size, categorical_cols, id_col)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# {'n_estimators': 300, 'learning_rate': 0.033266652862807916, 'max_depth': 10, 'min_samples_split': 7, 'subsample': 0.9566352087990605, 'max_features': 'log2', 'n_iter_no_change': 8, 'validation_fraction': 0.17499766883832907}\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Define and train the model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingSurvivalAnalysis(\n\u001b[1;32m     83\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m     84\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.033266652862807916\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Predict risk scores\u001b[39;00m\n\u001b[1;32m     98\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/boosting.py:1114\u001b[0m, in \u001b[0;36mGradientBoostingSurvivalAnalysis.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter(\u001b[38;5;28mzip\u001b[39m(event, time), dtype\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mbool\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mfloat64)])\n\u001b[0;32m-> 1114\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional tests)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/boosting.py:977\u001b[0m, in \u001b[0;36mGradientBoostingSurvivalAnalysis._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    970\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    971\u001b[0m         y_oob_sample,\n\u001b[1;32m    972\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    973\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/boosting.py:857\u001b[0m, in \u001b[0;36mGradientBoostingSurvivalAnalysis._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, scale, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    854\u001b[0m raw_predictions_copy \u001b[38;5;241m=\u001b[39m raw_predictions\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(loss\u001b[38;5;241m.\u001b[39mK):\n\u001b[0;32m--> 857\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# induce regression tree on residuals\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[1;32m    861\u001b[0m         criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion,\n\u001b[1;32m    862\u001b[0m         splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    871\u001b[0m         ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mccp_alpha,\n\u001b[1;32m    872\u001b[0m     )\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/survival_loss.py:65\u001b[0m, in \u001b[0;36mCoxPH.negative_gradient\u001b[0;34m(self, y, raw_predictions, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnegative_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, raw_predictions, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Negative gradient of partial likelihood\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        The predictions.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcoxph_negative_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m         ret \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m sample_weight\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "\n",
    "def prepare_data(df, categorical_cols, id_col='ID'):\n",
    "    # Create a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Ensure efs is integer (event indicator: 0 or 1)\n",
    "    data['efs'] = data['efs'].astype(int)\n",
    "    \n",
    "    # Drop the ID column if it exists\n",
    "    if id_col in data.columns:\n",
    "        data = data.drop(columns=[id_col])\n",
    "        print(f\"Dropped column: {id_col}\")\n",
    "    else:\n",
    "        print(f\"No column named '{id_col}' found in the dataset\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(['efs', 'efs_time'], axis=1)\n",
    "    y = Surv.from_arrays(event=data['efs'], time=data['efs_time'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define preprocessing for categorical and numerical columns\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names after one-hot encoding\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    feature_names = np.concatenate([cat_feature_names, numerical_cols])\n",
    "    \n",
    "    return X_preprocessed, y, feature_names, preprocessor\n",
    "\n",
    "def split_train_and_evaluate_with_sksurv(df, train_size=0.7, val_size=0.15, test_size=0.15,\n",
    "                                         categorical_cols=[\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "        'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate'\n",
    "    ], id_col='ID'):\n",
    "    assert train_size + val_size + test_size == 1.0, \"Split sizes must sum to 1\"\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, feature_names, preprocessor = prepare_data(df, categorical_cols, id_col)\n",
    "    print(feature_names)\n",
    "    \n",
    "    # Split into train + (val + test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Print sizes\n",
    "    print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X):.2%})\")\n",
    "    print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X):.2%})\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X):.2%})\")\n",
    "    \n",
    "    # {'n_estimators': 300, 'learning_rate': 0.033266652862807916, 'max_depth': 10, 'min_samples_split': 7, 'subsample': 0.9566352087990605, 'max_features': 'log2', 'n_iter_no_change': 8, 'validation_fraction': 0.17499766883832907}\n",
    "    \n",
    "    # Define and train the model\n",
    "    model = GradientBoostingSurvivalAnalysis(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.033266652862807916,\n",
    "        max_depth=10,\n",
    "        min_samples_split=7,\n",
    "        subsample=0.9566352087990605,\n",
    "        random_state=42,\n",
    "        max_features='log2', # 'sqrt', 0.3-0.7, \n",
    "        n_iter_no_change=8, # Set to 10, 20, or 50, and pair with a validation fraction\n",
    "        validation_fraction=0.17499766883832907, # 0.1â€“0.3\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict risk scores\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Extract event times and indicators for evaluation\n",
    "    t_train = y_train['time']\n",
    "    e_train = y_train['event']\n",
    "\n",
    "    t_test = y_test['time']\n",
    "    e_test = y_test['event']\n",
    "    \n",
    "    print(\"Test prediction head: \")\n",
    "    print(test_pred[:5])\n",
    "\n",
    "    \n",
    "    # Calculate C-index\n",
    "    c_index_train = concordance_index(t_train, train_pred, e_train)\n",
    "    c_index_test = concordance_index(t_test, test_pred, e_test)\n",
    "    print(f\"\\nTraining C-index: {c_index_train:.4f}\")\n",
    "    print(f\"Test C-index: {c_index_test:.4f}\")\n",
    "    \n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    return model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test, val_pred, test_pred\n",
    "\n",
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "# df['conditioning_intensity'] = df['conditioning_intensity'].fillna(\"TBD\")\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test, val_pred, test_pred = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.75,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       62287.2491          16.7188           11.12m\n",
      "         2       62099.0223          16.8461           10.97m\n",
      "         3       61801.5990          10.6282           10.92m\n",
      "         4       61816.3984          12.7198           10.91m\n",
      "         5       61975.0881          15.7763           11.10m\n",
      "         6       62162.4589          12.8983           11.08m\n",
      "         7       61982.1026          12.8914           11.15m\n",
      "         8       62267.3699          12.4272           10.96m\n",
      "         9       60894.6215           8.6825           10.80m\n",
      "        10       62069.7516           9.9121           10.76m\n",
      "        20       61479.0065           9.3756            9.77m\n",
      "        30       61627.0932           5.4219            8.96m\n",
      "        40       61141.8838           4.6263            8.22m\n",
      "        50       61664.3569           4.1940            7.44m\n",
      "        60       61229.9877           3.1116            6.69m\n",
      "        70       61297.6371           1.3924            5.92m\n",
      "        80       60542.8210           2.0759            5.18m\n",
      "        90       60864.8215           2.0093            4.43m\n",
      "       100       60540.9061           1.3040            3.67m\n",
      "Test prediction head: \n",
      "[-0.42799526 -0.09528771 -0.08764094  0.10464633 -0.15750619]\n",
      "\n",
      "Training C-index: 0.2633\n",
      "Validation C-index: 0.3242\n",
      "Test C-index: 0.3337\n",
      "Test Set ROC-AUC: 0.7330\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.050037\n",
      "199               comorbidity_score    0.046023\n",
      "0                    dri_score_High    0.041309\n",
      "196                      age_at_hct    0.041294\n",
      "192                        year_hct    0.035316\n",
      "92       conditioning_intensity_RIC    0.033944\n",
      "194                       donor_age    0.026312\n",
      "1                     dri_score_Low    0.024679\n",
      "200                 karnofsky_score    0.024307\n",
      "85           cyto_score_detail_Poor    0.019402\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdri_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdri_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(bin_dri_score)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Call the function, specifying the ID column to remove\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test \u001b[38;5;241m=\u001b[39m split_train_and_evaluate_with_sksurv(\n\u001b[1;32m     30\u001b[0m     df,\n\u001b[1;32m     31\u001b[0m     train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     32\u001b[0m     val_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m     33\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m     34\u001b[0m     id_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust this to match your actual ID column name\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 8)"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Define the mapping for the bins\n",
    "dri_bins = {\n",
    "    'High': ['High'],\n",
    "    'Medium': ['Intermediate', 'High - TED AML case <missing cytogenetics', \n",
    "               'Intermediate - TED AML case <missing cytogenetics', 'Low', \n",
    "               'Missing disease status'],\n",
    "    'Low': ['N/A - disease not classifiable', 'N/A - non-malignant indication', \n",
    "            'N/A - pediatric', 'TBD cytogenetics', 'Very high']\n",
    "}\n",
    "\n",
    "# Function to map dri_score to new bins\n",
    "def bin_dri_score(score):\n",
    "    if pd.isna(score):  # Handle NaN values\n",
    "        return 'Low'  # Assuming NaN goes to 'Low', adjust if needed\n",
    "    for bin_name, values in dri_bins.items():\n",
    "        if score in values:\n",
    "            return bin_name\n",
    "    return 'Low'  # Default for any unmapped values (e.g., edge cases)\n",
    "\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       62295.9481          14.7725           11.03m\n",
      "         2       62110.7400          16.7796           10.88m\n",
      "         3       61806.2100          12.1696           10.71m\n",
      "         4       61828.2801          11.5978           10.58m\n",
      "         5       61989.1790          15.0982           10.48m\n",
      "         6       62169.9494          11.3840           10.40m\n",
      "         7       62000.6606          12.4961           10.32m\n",
      "         8       62283.0488          11.4669           10.25m\n",
      "         9       60904.3259          13.3742           10.17m\n",
      "        10       62081.9527          11.9501           10.09m\n",
      "        20       61507.7037           8.4885            9.33m\n",
      "        30       61646.0139           5.4452            8.64m\n",
      "        40       61164.9362           4.4419            7.90m\n",
      "        50       61674.2985           4.5870            7.20m\n",
      "        60       61238.5733           2.7627            6.47m\n",
      "        70       61308.3480           2.2587            5.76m\n",
      "        80       60567.5892           1.8780            5.06m\n",
      "        90       60889.6481           0.7771            4.35m\n",
      "       100       60571.0013           1.1686            3.68m\n",
      "\n",
      "Training C-index: 0.7363\n",
      "Validation C-index: 0.6749\n",
      "Test C-index: 0.6673\n",
      "Test Set ROC-AUC: 0.7323\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "97   conditioning_intensity_Missing    0.060985\n",
      "208               comorbidity_score    0.048335\n",
      "0                    dri_score_High    0.045380\n",
      "201                        year_hct    0.035329\n",
      "205                      age_at_hct    0.035042\n",
      "101      conditioning_intensity_RIC    0.029759\n",
      "203                       donor_age    0.026247\n",
      "209                 karnofsky_score    0.025236\n",
      "92        cyto_score_detail_Missing    0.021988\n",
      "18               cyto_score_Missing    0.020073\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "# df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "\n",
    "\n",
    "def categorize_hla_by_percantile(df):\n",
    "    \n",
    "    hla_features = ['hla_high_res_8', 'hla_match_a_high', 'hla_match_b_high', 'hla_low_res_6']\n",
    "\n",
    "    # Function to categorize based on 25th percentile\n",
    "    def categorize_by_percentile(series):\n",
    "        threshold = series.quantile(0.25)  # Calculate the 25th percentile\n",
    "        return np.where(series <= threshold, 0, 1)  # 0 if <= threshold, 1 if above\n",
    "\n",
    "    # Convert each HLA feature to categorical (0 or 1)\n",
    "    for feature in hla_features:\n",
    "        # new_column = f\"{feature}_cat\"  # Create a new column name for the categorical version\n",
    "        df[feature] = categorize_by_percentile(df[feature])\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "\n",
    "    \n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.3388           13.98m\n",
      "         2       69998.0170          24.1502           13.86m\n",
      "         3       69726.1121          21.8861           13.69m\n",
      "         4       69891.5344          21.9805           13.57m\n",
      "         5       70081.8364          19.9264           13.45m\n",
      "         6       69508.5773          20.0008           13.34m\n",
      "         7       69415.6389          17.6574           13.24m\n",
      "         8       70364.2840          16.7081           13.14m\n",
      "         9       69661.6153          15.9291           13.20m\n",
      "        10       69502.7763          15.3668           13.21m\n",
      "        20       69211.9050           7.2304           12.19m\n",
      "        30       68485.2945           5.9317           11.19m\n",
      "        40       69326.4869           3.1606           10.25m\n",
      "        50       68113.9842           2.0456            9.34m\n",
      "        60       68303.9589           1.6006            8.40m\n",
      "        70       68465.3898           1.5435            7.46m\n",
      "        80       68449.4899           1.4696            6.48m\n",
      "        90       68346.2302           1.0787            5.55m\n",
      "       100       67828.1251           1.1476            4.62m\n",
      "\n",
      "Training C-index: 0.7275\n",
      "Validation C-index: 0.6800\n",
      "Test C-index: 0.6684\n",
      "Test Set ROC-AUC: 0.7316\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.201541\n",
      "199               comorbidity_score    0.061444\n",
      "196                      age_at_hct    0.060788\n",
      "0                    dri_score_High    0.055100\n",
      "192                        year_hct    0.052987\n",
      "194                       donor_age    0.049271\n",
      "200                 karnofsky_score    0.034752\n",
      "92       conditioning_intensity_RIC    0.027285\n",
      "85           cyto_score_detail_Poor    0.021969\n",
      "150                   sex_match_M-M    0.015302\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "\n",
    "df['pediatric_and_arrhythmia'] = ((df['dri_score'] == 'N/A - pediatric') & (df['arrhythmia'] == 'Yes')).astype(int)\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.8190          25.4356           13.54m\n",
      "         2       69998.2960          24.1827           13.53m\n",
      "         3       69726.6986          22.2303           13.43m\n",
      "         4       69891.9708          21.6603           13.32m\n",
      "         5       70082.3080          19.7763           13.26m\n",
      "         6       69508.6662          19.9060           13.16m\n",
      "         7       69415.7453          17.7379           13.08m\n",
      "         8       70365.4704          16.4794           12.99m\n",
      "         9       69662.3624          15.8926           12.90m\n",
      "        10       69503.8917          15.3288           12.81m\n",
      "        20       69214.0825           7.2036           11.88m\n",
      "        30       68487.4745           6.1151           10.95m\n",
      "        40       69325.7869           3.3005           10.05m\n",
      "        50       68112.7289           2.3340            9.13m\n",
      "        60       68304.2369           2.1113            8.24m\n",
      "        70       68469.5511           1.2215            7.34m\n",
      "        80       68454.6619           1.4328            6.43m\n",
      "        90       68352.9919           0.7823            5.51m\n",
      "       100       67833.3218           1.1991            4.60m\n",
      "\n",
      "Training C-index: 0.7261\n",
      "Validation C-index: 0.6782\n",
      "Test C-index: 0.6692\n",
      "Test Set ROC-AUC: 0.7324\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.202769\n",
      "193               comorbidity_score    0.061686\n",
      "190                      age_at_hct    0.061069\n",
      "0                    dri_score_High    0.055768\n",
      "186                        year_hct    0.053256\n",
      "188                       donor_age    0.051182\n",
      "194                 karnofsky_score    0.035218\n",
      "92       conditioning_intensity_RIC    0.027390\n",
      "85           cyto_score_detail_Poor    0.022368\n",
      "150                   sex_match_M-M    0.015623\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Define the mapping for the bins\n",
    "dri_bins = {\n",
    "    'High': ['High'],\n",
    "    'Medium': ['Intermediate', 'High - TED AML case <missing cytogenetics', \n",
    "               'Intermediate - TED AML case <missing cytogenetics', 'Low', \n",
    "               'Missing disease status'],\n",
    "    'Low': ['N/A - disease not classifiable', 'N/A - non-malignant indication', \n",
    "            'N/A - pediatric', 'TBD cytogenetics', 'Very high']\n",
    "}\n",
    "\n",
    "# Function to map dri_score to new bins\n",
    "def bin_dri_score(score):\n",
    "    if pd.isna(score):  # Handle NaN values\n",
    "        return 'Low'  # Assuming NaN goes to 'Low', adjust if needed\n",
    "    for bin_name, values in dri_bins.items():\n",
    "        if score in values:\n",
    "            return bin_name\n",
    "    return 'Low'  # Default for any unmapped values (e.g., edge cases)\n",
    "\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df = df.drop([\"race_group\"], axis=1)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID',  # Adjust this to match your actual ID column name\n",
    "    categorical_cols = [\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
