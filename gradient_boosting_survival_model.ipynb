{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 21600 (75.00%)\n",
      "Validation set size: 2880 (10.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       75577.4952          31.9504           16.06m\n",
      "         2       75678.3447          27.6487           16.28m\n",
      "         3       75825.7507          26.3442           15.96m\n",
      "         4       75653.7739          25.3938           15.65m\n",
      "         5       75098.3863          23.1405           15.50m\n",
      "         6       75205.2447          22.0882           15.36m\n",
      "         7       75199.7085          21.3058           15.19m\n",
      "         8       75202.5603          19.8694           15.07m\n",
      "         9       75134.2842          20.1340           14.96m\n",
      "        10       75257.0674          17.7319           14.91m\n",
      "        20       74743.8373          10.5010           13.87m\n",
      "        30       73795.7747           7.0241           12.83m\n",
      "        40       73363.5955           4.7301           11.75m\n",
      "        50       73475.6927           2.9241           10.65m\n",
      "        60       73526.5663           2.1485            9.59m\n",
      "        70       73139.9786          -0.2859            8.57m\n",
      "        80       73245.7411           0.9674            7.51m\n",
      "        90       73072.5818           1.2174            6.44m\n",
      "       100       72772.9966           0.5598            5.36m\n",
      "\n",
      "Training C-index: 0.7897\n",
      "Validation C-index: 0.6822\n",
      "Test C-index: 0.6671\n",
      "Test Set ROC-AUC: 0.7273\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "97   conditioning_intensity_Missing    0.132096\n",
      "205                      age_at_hct    0.074856\n",
      "203                       donor_age    0.066778\n",
      "208               comorbidity_score    0.049808\n",
      "201                        year_hct    0.047994\n",
      "0                    dri_score_High    0.040332\n",
      "209                 karnofsky_score    0.031143\n",
      "101      conditioning_intensity_RIC    0.017938\n",
      "94           cyto_score_detail_Poor    0.014459\n",
      "159                   sex_match_M-M    0.012445\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "def prepare_data(df, categorical_cols, id_col='ID'):\n",
    "    # Create a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Ensure efs is integer (event indicator: 0 or 1)\n",
    "    data['efs'] = data['efs'].astype(int)\n",
    "    \n",
    "    # Drop the ID column if it exists\n",
    "    if id_col in data.columns:\n",
    "        data = data.drop(columns=[id_col])\n",
    "        print(f\"Dropped column: {id_col}\")\n",
    "    else:\n",
    "        print(f\"No column named '{id_col}' found in the dataset\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(['efs', 'efs_time'], axis=1)\n",
    "    y = Surv.from_arrays(event=data['efs'], time=data['efs_time'])\n",
    "    \n",
    "    # Define preprocessing for categorical and numerical columns\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names after one-hot encoding\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    feature_names = np.concatenate([cat_feature_names, numerical_cols])\n",
    "    \n",
    "    return X_preprocessed, y, feature_names, preprocessor\n",
    "\n",
    "def split_train_and_evaluate_with_sksurv(df, train_size=0.7, val_size=0.15, test_size=0.15,\n",
    "                                         categorical_cols=[\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "        'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate'\n",
    "    ], id_col='ID'):\n",
    "    assert train_size + val_size + test_size == 1.0, \"Split sizes must sum to 1\"\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, feature_names, preprocessor = prepare_data(df, categorical_cols, id_col)\n",
    "    \n",
    "    # Split into train + (val + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(val_size + test_size), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(1 - val_proportion), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print sizes\n",
    "    print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X):.2%})\")\n",
    "    print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X):.2%})\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X):.2%})\")\n",
    "    \n",
    "    # Define and train the model\n",
    "    model = GradientBoostingSurvivalAnalysis(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.06,\n",
    "        max_depth=8,\n",
    "        min_samples_split=4,\n",
    "        subsample=0.7,\n",
    "        random_state=42,\n",
    "        max_features='log2', # 'sqrt', 0.3-0.7, \n",
    "        n_iter_no_change=10, # Set to 10, 20, or 50, and pair with a validation fraction\n",
    "        validation_fraction=0.1, # 0.1â€“0.3\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict risk scores\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Extract event times and indicators for evaluation\n",
    "    t_train = y_train['time']\n",
    "    e_train = y_train['event']\n",
    "    t_val = y_val['time']\n",
    "    e_val = y_val['event']\n",
    "    t_test = y_test['time']\n",
    "    e_test = y_test['event']\n",
    "    \n",
    "    # Calculate C-index\n",
    "    c_index_train = concordance_index(t_train, -train_pred, e_train)\n",
    "    c_index_val = concordance_index(t_val, -val_pred, e_val)\n",
    "    c_index_test = concordance_index(t_test, -test_pred, e_test)\n",
    "    print(f\"\\nTraining C-index: {c_index_train:.4f}\")\n",
    "    print(f\"Validation C-index: {c_index_val:.4f}\")\n",
    "    print(f\"Test C-index: {c_index_test:.4f}\")\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc_test = roc_auc_score(e_test, test_pred)\n",
    "    print(f\"Test Set ROC-AUC: {roc_auc_test:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    return model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.75,\n",
    "    val_size=0.10,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.3387           13.83m\n",
      "         2       69998.1983          24.1738           13.74m\n",
      "         3       69726.3241          21.9602           13.64m\n",
      "         4       69891.6005          21.9284           13.52m\n",
      "         5       70081.8751          19.8657           13.46m\n",
      "         6       69508.4244          19.8745           13.36m\n",
      "         7       69415.7921          17.5501           13.25m\n",
      "         8       70364.4059          16.5063           13.16m\n",
      "         9       69662.1926          16.0336           13.06m\n",
      "        10       69502.9004          15.4836           12.97m\n",
      "        20       69211.3332           7.2526           12.04m\n",
      "        30       68485.3863           5.7107           11.17m\n",
      "        40       69326.0684           3.5123           10.27m\n",
      "        50       68114.7748           1.9796            9.37m\n",
      "        60       68303.4062           1.6303            8.47m\n",
      "        70       68464.6766           1.1825            7.55m\n",
      "        80       68451.1882           1.4202            6.59m\n",
      "        90       68349.0108           1.1893            5.67m\n",
      "       100       67833.4589           0.4104            4.74m\n",
      "\n",
      "Training C-index: 0.7268\n",
      "Validation C-index: 0.6798\n",
      "Test C-index: 0.6694\n",
      "Test Set ROC-AUC: 0.7332\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.201977\n",
      "199               comorbidity_score    0.061764\n",
      "196                      age_at_hct    0.059175\n",
      "0                    dri_score_High    0.055443\n",
      "192                        year_hct    0.052506\n",
      "194                       donor_age    0.048244\n",
      "200                 karnofsky_score    0.034686\n",
      "92       conditioning_intensity_RIC    0.026881\n",
      "85           cyto_score_detail_Poor    0.021742\n",
      "150                   sex_match_M-M    0.015370\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Define the mapping for the bins\n",
    "dri_bins = {\n",
    "    'High': ['High'],\n",
    "    'Medium': ['Intermediate', 'High - TED AML case <missing cytogenetics', \n",
    "               'Intermediate - TED AML case <missing cytogenetics', 'Low', \n",
    "               'Missing disease status'],\n",
    "    'Low': ['N/A - disease not classifiable', 'N/A - non-malignant indication', \n",
    "            'N/A - pediatric', 'TBD cytogenetics', 'Very high']\n",
    "}\n",
    "\n",
    "# Function to map dri_score to new bins\n",
    "def bin_dri_score(score):\n",
    "    if pd.isna(score):  # Handle NaN values\n",
    "        return 'Low'  # Assuming NaN goes to 'Low', adjust if needed\n",
    "    for bin_name, values in dri_bins.items():\n",
    "        if score in values:\n",
    "            return bin_name\n",
    "    return 'Low'  # Default for any unmapped values (e.g., edge cases)\n",
    "\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.3388           13.63m\n",
      "         2       69998.1983          24.1876           13.58m\n",
      "         3       69726.2457          21.9754           13.70m\n",
      "         4       69891.5447          22.1225           13.83m\n",
      "         5       70081.8732          19.6981           14.09m\n",
      "         6       69508.3196          19.5528           13.87m\n",
      "         7       69415.7827          17.4511           13.84m\n",
      "         8       70364.7218          16.4501           13.65m\n",
      "         9       69662.3271          16.0417           13.52m\n",
      "        10       69502.5947          15.1632           13.37m\n",
      "        20       69211.7872           7.2756           12.15m\n",
      "        30       68483.6621           6.1609           11.20m\n",
      "        40       69322.7656           3.0109           10.25m\n",
      "        50       68110.6209           2.0708            9.35m\n",
      "        60       68298.3246           1.8962            8.42m\n",
      "        70       68459.4146           1.5928            7.50m\n",
      "        80       68440.1526           1.4209            6.55m\n",
      "        90       68338.6252           1.1767            5.61m\n",
      "       100       67821.6714           0.2180            4.67m\n",
      "\n",
      "Training C-index: 0.7286\n",
      "Validation C-index: 0.6794\n",
      "Test C-index: 0.6704\n",
      "Test Set ROC-AUC: 0.7337\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "97   conditioning_intensity_Missing    0.201201\n",
      "208               comorbidity_score    0.060258\n",
      "205                      age_at_hct    0.059690\n",
      "0                    dri_score_High    0.054167\n",
      "201                        year_hct    0.053091\n",
      "203                       donor_age    0.048002\n",
      "209                 karnofsky_score    0.034128\n",
      "101      conditioning_intensity_RIC    0.027169\n",
      "94           cyto_score_detail_Poor    0.021794\n",
      "159                   sex_match_M-M    0.015001\n",
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.2636           15.19m\n",
      "         2       69998.0171          24.1979           14.69m\n",
      "         3       69726.1562          22.0884           14.33m\n",
      "         4       69891.3492          21.9718           14.04m\n",
      "         5       70081.7561          19.7124           13.83m\n",
      "         6       69508.3314          19.3225           13.75m\n",
      "         7       69415.7026          17.2575           13.65m\n",
      "         8       70364.7548          16.7999           13.64m\n",
      "         9       69662.9292          16.0959           13.58m\n",
      "        10       69503.1998          15.1556           13.56m\n",
      "        20       69211.2807           7.4462           12.59m\n",
      "        30       68483.0056           5.7505           11.64m\n",
      "        40       69321.8550           2.9800           10.61m\n",
      "        50       68109.4819           2.2614            9.55m\n",
      "        60       68297.6725           1.9035            8.59m\n",
      "        70       68458.9149           1.4057            7.64m\n",
      "        80       68444.5576           1.4189            6.69m\n",
      "        90       68338.1455           0.8443            5.74m\n",
      "       100       67820.1018           1.0604            4.78m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n\u001b[1;32m     34\u001b[0m df \u001b[38;5;241m=\u001b[39m categorize_hla_by_percantile(df)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Call the function, specifying the ID column to remove\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_train_and_evaluate_with_sksurv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust this to match your actual ID column name\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 103\u001b[0m, in \u001b[0;36msplit_train_and_evaluate_with_sksurv\u001b[0;34m(df, train_size, val_size, test_size, categorical_cols, id_col)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Define and train the model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingSurvivalAnalysis(\n\u001b[1;32m     94\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     95\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.06\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m )\n\u001b[0;32m--> 103\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Predict risk scores\u001b[39;00m\n\u001b[1;32m    106\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/boosting.py:1114\u001b[0m, in \u001b[0;36mGradientBoostingSurvivalAnalysis.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter(\u001b[38;5;28mzip\u001b[39m(event, time), dtype\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mbool\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mfloat64)])\n\u001b[0;32m-> 1114\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional tests)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/boosting.py:992\u001b[0m, in \u001b[0;36mGradientBoostingSurvivalAnalysis._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n\u001b[0;32m--> 992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_score_[i] \u001b[38;5;241m=\u001b[39m \u001b[43mloss_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_improvement_[i] \u001b[38;5;241m=\u001b[39m old_oob_score \u001b[38;5;241m-\u001b[39m loss_(\n\u001b[1;32m    998\u001b[0m         y_oob_sample,\n\u001b[1;32m    999\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m   1000\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m   1001\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# no need to fancy index w/ no subsampling\u001b[39;00m\n",
      "File \u001b[0;32m~/kaggle/CIBMTR-Equity-in-post-HCT-Survival-Predictions/venv/lib/python3.8/site-packages/sksurv/ensemble/survival_loss.py:53\u001b[0m, in \u001b[0;36mCoxPH.__call__\u001b[0;34m(self, y, raw_predictions, sample_weight)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the partial likelihood of prediction ``y_pred`` and ``y``.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# TODO add support for sample weights\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcoxph_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "# df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")\n",
    "\n",
    "def categorize_hla_by_percantile(df):\n",
    "    \n",
    "    hla_features = ['hla_high_res_8', 'hla_match_a_high', 'hla_match_b_high', 'hla_low_res_6']\n",
    "\n",
    "    # Function to categorize based on 25th percentile\n",
    "    def categorize_by_percentile(series):\n",
    "        threshold = series.quantile(0.25)  # Calculate the 25th percentile\n",
    "        return np.where(series <= threshold, 0, 1)  # 0 if <= threshold, 1 if above\n",
    "\n",
    "    # Convert each HLA feature to categorical (0 or 1)\n",
    "    for feature in hla_features:\n",
    "        # new_column = f\"{feature}_cat\"  # Create a new column name for the categorical version\n",
    "        df[feature] = categorize_by_percentile(df[feature])\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "\n",
    "    \n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.3388           13.98m\n",
      "         2       69998.0170          24.1502           13.86m\n",
      "         3       69726.1121          21.8861           13.69m\n",
      "         4       69891.5344          21.9805           13.57m\n",
      "         5       70081.8364          19.9264           13.45m\n",
      "         6       69508.5773          20.0008           13.34m\n",
      "         7       69415.6389          17.6574           13.24m\n",
      "         8       70364.2840          16.7081           13.14m\n",
      "         9       69661.6153          15.9291           13.20m\n",
      "        10       69502.7763          15.3668           13.21m\n",
      "        20       69211.9050           7.2304           12.19m\n",
      "        30       68485.2945           5.9317           11.19m\n",
      "        40       69326.4869           3.1606           10.25m\n",
      "        50       68113.9842           2.0456            9.34m\n",
      "        60       68303.9589           1.6006            8.40m\n",
      "        70       68465.3898           1.5435            7.46m\n",
      "        80       68449.4899           1.4696            6.48m\n",
      "        90       68346.2302           1.0787            5.55m\n",
      "       100       67828.1251           1.1476            4.62m\n",
      "\n",
      "Training C-index: 0.7275\n",
      "Validation C-index: 0.6800\n",
      "Test C-index: 0.6684\n",
      "Test Set ROC-AUC: 0.7316\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.201541\n",
      "199               comorbidity_score    0.061444\n",
      "196                      age_at_hct    0.060788\n",
      "0                    dri_score_High    0.055100\n",
      "192                        year_hct    0.052987\n",
      "194                       donor_age    0.049271\n",
      "200                 karnofsky_score    0.034752\n",
      "92       conditioning_intensity_RIC    0.027285\n",
      "85           cyto_score_detail_Poor    0.021969\n",
      "150                   sex_match_M-M    0.015302\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "\n",
    "df['pediatric_and_arrhythmia'] = ((df['dri_score'] == 'N/A - pediatric') & (df['arrhythmia'] == 'Yes')).astype(int)\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.8190          25.4356           13.54m\n",
      "         2       69998.2960          24.1827           13.53m\n",
      "         3       69726.6986          22.2303           13.43m\n",
      "         4       69891.9708          21.6603           13.32m\n",
      "         5       70082.3080          19.7763           13.26m\n",
      "         6       69508.6662          19.9060           13.16m\n",
      "         7       69415.7453          17.7379           13.08m\n",
      "         8       70365.4704          16.4794           12.99m\n",
      "         9       69662.3624          15.8926           12.90m\n",
      "        10       69503.8917          15.3288           12.81m\n",
      "        20       69214.0825           7.2036           11.88m\n",
      "        30       68487.4745           6.1151           10.95m\n",
      "        40       69325.7869           3.3005           10.05m\n",
      "        50       68112.7289           2.3340            9.13m\n",
      "        60       68304.2369           2.1113            8.24m\n",
      "        70       68469.5511           1.2215            7.34m\n",
      "        80       68454.6619           1.4328            6.43m\n",
      "        90       68352.9919           0.7823            5.51m\n",
      "       100       67833.3218           1.1991            4.60m\n",
      "\n",
      "Training C-index: 0.7261\n",
      "Validation C-index: 0.6782\n",
      "Test C-index: 0.6692\n",
      "Test Set ROC-AUC: 0.7324\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "88   conditioning_intensity_Missing    0.202769\n",
      "193               comorbidity_score    0.061686\n",
      "190                      age_at_hct    0.061069\n",
      "0                    dri_score_High    0.055768\n",
      "186                        year_hct    0.053256\n",
      "188                       donor_age    0.051182\n",
      "194                 karnofsky_score    0.035218\n",
      "92       conditioning_intensity_RIC    0.027390\n",
      "85           cyto_score_detail_Poor    0.022368\n",
      "150                   sex_match_M-M    0.015623\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Define the mapping for the bins\n",
    "dri_bins = {\n",
    "    'High': ['High'],\n",
    "    'Medium': ['Intermediate', 'High - TED AML case <missing cytogenetics', \n",
    "               'Intermediate - TED AML case <missing cytogenetics', 'Low', \n",
    "               'Missing disease status'],\n",
    "    'Low': ['N/A - disease not classifiable', 'N/A - non-malignant indication', \n",
    "            'N/A - pediatric', 'TBD cytogenetics', 'Very high']\n",
    "}\n",
    "\n",
    "# Function to map dri_score to new bins\n",
    "def bin_dri_score(score):\n",
    "    if pd.isna(score):  # Handle NaN values\n",
    "        return 'Low'  # Assuming NaN goes to 'Low', adjust if needed\n",
    "    for bin_name, values in dri_bins.items():\n",
    "        if score in values:\n",
    "            return bin_name\n",
    "    return 'Low'  # Default for any unmapped values (e.g., edge cases)\n",
    "\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df = df.drop([\"race_group\"], axis=1)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID',  # Adjust this to match your actual ID column name\n",
    "    categorical_cols = [\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
