{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.2636           13.60m\n",
      "         2       69998.1985          24.1577           13.56m\n",
      "         3       69726.3468          22.0896           13.42m\n",
      "         4       69891.4592          22.1185           13.34m\n",
      "         5       70081.9624          19.7891           13.23m\n",
      "         6       69508.2294          19.6673           13.13m\n",
      "         7       69416.6861          17.5869           13.03m\n",
      "         8       70365.6406          16.4882           12.93m\n",
      "         9       69662.8971          16.0406           12.87m\n",
      "        10       69503.1248          15.1158           12.82m\n",
      "        20       69211.9961           7.3247           11.90m\n",
      "        30       68482.2884           5.7817           10.99m\n",
      "        40       69322.6703           2.9959           10.11m\n",
      "        50       68110.3083           2.0882            9.23m\n",
      "        60       68300.3557           2.1308            8.36m\n",
      "        70       68461.7488           1.5331            7.43m\n",
      "        80       68443.3696           0.6779            6.50m\n",
      "        90       68335.0743           0.4518            5.58m\n",
      "       100       67818.6493           1.2806            4.65m\n",
      "\n",
      "Training C-index: 0.7293\n",
      "Validation C-index: 0.6797\n",
      "Test C-index: 0.6694\n",
      "Test Set ROC-AUC: 0.7320\n",
      "\n",
      "Top 10 most important features:\n",
      "                            feature  importance\n",
      "97   conditioning_intensity_Missing    0.200516\n",
      "208               comorbidity_score    0.060829\n",
      "205                      age_at_hct    0.059840\n",
      "0                    dri_score_High    0.054160\n",
      "201                        year_hct    0.052756\n",
      "203                       donor_age    0.046253\n",
      "209                 karnofsky_score    0.034080\n",
      "101      conditioning_intensity_RIC    0.027386\n",
      "94           cyto_score_detail_Poor    0.022012\n",
      "159                   sex_match_M-M    0.015406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "def prepare_data(df, categorical_cols, id_col='ID'):\n",
    "    # Create a copy of the dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Ensure efs is integer (event indicator: 0 or 1)\n",
    "    data['efs'] = data['efs'].astype(int)\n",
    "    \n",
    "    # Drop the ID column if it exists\n",
    "    if id_col in data.columns:\n",
    "        data = data.drop(columns=[id_col])\n",
    "        print(f\"Dropped column: {id_col}\")\n",
    "    else:\n",
    "        print(f\"No column named '{id_col}' found in the dataset\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(['efs', 'efs_time'], axis=1)\n",
    "    y = Surv.from_arrays(event=data['efs'], time=data['efs_time'])\n",
    "    \n",
    "    # Define preprocessing for categorical and numerical columns\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names after one-hot encoding\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    feature_names = np.concatenate([cat_feature_names, numerical_cols])\n",
    "    \n",
    "    return X_preprocessed, y, feature_names, preprocessor\n",
    "\n",
    "def split_train_and_evaluate_with_sksurv(df, train_size=0.7, val_size=0.15, test_size=0.15,\n",
    "                                         categorical_cols=[\n",
    "        'dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status',\n",
    "        'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe',\n",
    "        'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab',\n",
    "        'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity',\n",
    "        'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "        'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "        'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related',\n",
    "        'melphalan_dose', 'cardiac', 'pulm_moderate'\n",
    "    ], id_col='ID'):\n",
    "    assert train_size + val_size + test_size == 1.0, \"Split sizes must sum to 1\"\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, feature_names, preprocessor = prepare_data(df, categorical_cols, id_col)\n",
    "    \n",
    "    # Split into train + (val + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(val_size + test_size), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(1 - val_proportion), random_state=42\n",
    "    )\n",
    "    \n",
    "    # Print sizes\n",
    "    print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X):.2%})\")\n",
    "    print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(X):.2%})\")\n",
    "    print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X):.2%})\")\n",
    "    \n",
    "    # Define and train the model\n",
    "    model = GradientBoostingSurvivalAnalysis(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.06,\n",
    "        max_depth=6,\n",
    "        min_samples_split=4,\n",
    "        subsample=0.7,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict risk scores\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Extract event times and indicators for evaluation\n",
    "    t_train = y_train['time']\n",
    "    e_train = y_train['event']\n",
    "    t_val = y_val['time']\n",
    "    e_val = y_val['event']\n",
    "    t_test = y_test['time']\n",
    "    e_test = y_test['event']\n",
    "    \n",
    "    # Calculate C-index\n",
    "    c_index_train = concordance_index(t_train, -train_pred, e_train)\n",
    "    c_index_val = concordance_index(t_val, -val_pred, e_val)\n",
    "    c_index_test = concordance_index(t_test, -test_pred, e_test)\n",
    "    print(f\"\\nTraining C-index: {c_index_train:.4f}\")\n",
    "    print(f\"Validation C-index: {c_index_val:.4f}\")\n",
    "    print(f\"Test C-index: {c_index_test:.4f}\")\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc_test = roc_auc_score(e_test, test_pred)\n",
    "    print(f\"Test Set ROC-AUC: {roc_auc_test:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    return model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: ID\n",
      "Training set size: 20160 (70.00%)\n",
      "Validation set size: 4320 (15.00%)\n",
      "Test set size: 4320 (15.00%)\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       70358.7919          25.3387           13.83m\n",
      "         2       69998.1983          24.1738           13.74m\n",
      "         3       69726.3241          21.9602           13.64m\n",
      "         4       69891.6005          21.9284           13.52m\n",
      "         5       70081.8751          19.8657           13.46m\n",
      "         6       69508.4244          19.8745           13.36m\n",
      "         7       69415.7921          17.5501           13.25m\n",
      "         8       70364.4059          16.5063           13.16m\n",
      "         9       69662.1926          16.0336           13.06m\n",
      "        10       69502.9004          15.4836           12.97m\n",
      "        20       69211.3332           7.2526           12.04m\n",
      "        30       68485.3863           5.7107           11.17m\n",
      "        40       69326.0684           3.5123           10.27m\n",
      "        50       68114.7748           1.9796            9.37m\n",
      "        60       68303.4062           1.6303            8.47m\n",
      "        70       68464.6766           1.1825            7.55m\n",
      "        80       68451.1882           1.4202            6.59m\n",
      "        90       68349.0108           1.1893            5.67m\n",
      "       100       67833.4589           0.4104            4.74m\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the model\n",
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "# Define the mapping for the bins\n",
    "dri_bins = {\n",
    "    'High': ['High'],\n",
    "    'Medium': ['Intermediate', 'High - TED AML case <missing cytogenetics', \n",
    "               'Intermediate - TED AML case <missing cytogenetics', 'Low', \n",
    "               'Missing disease status'],\n",
    "    'Low': ['N/A - disease not classifiable', 'N/A - non-malignant indication', \n",
    "            'N/A - pediatric', 'TBD cytogenetics', 'Very high']\n",
    "}\n",
    "\n",
    "# Function to map dri_score to new bins\n",
    "def bin_dri_score(score):\n",
    "    if pd.isna(score):  # Handle NaN values\n",
    "        return 'Low'  # Assuming NaN goes to 'Low', adjust if needed\n",
    "    for bin_name, values in dri_bins.items():\n",
    "        if score in values:\n",
    "            return bin_name\n",
    "    return 'Low'  # Default for any unmapped values (e.g., edge cases)\n",
    "\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "# df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")\n",
    "\n",
    "def categorize_hla_by_percantile(df):\n",
    "    \n",
    "    hla_features = ['hla_high_res_8', 'hla_match_a_high', 'hla_match_b_high', 'hla_low_res_6']\n",
    "\n",
    "    # Function to categorize based on 25th percentile\n",
    "    def categorize_by_percentile(series):\n",
    "        threshold = series.quantile(0.25)  # Calculate the 25th percentile\n",
    "        return np.where(series <= threshold, 0, 1)  # 0 if <= threshold, 1 if above\n",
    "\n",
    "    # Convert each HLA feature to categorical (0 or 1)\n",
    "    for feature in hla_features:\n",
    "        # new_column = f\"{feature}_cat\"  # Create a new column name for the categorical version\n",
    "        df[feature] = categorize_by_percentile(df[feature])\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "\n",
    "    \n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/equity-post-HCT-survival-predictions/train.csv')\n",
    "\n",
    "df['dri_score'] = df['dri_score'].apply(bin_dri_score)\n",
    "\n",
    "df['has_hodgekins'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HD' else 0)\n",
    "df['has_hemophagocyticImmuneSyndrome'] = df['prim_disease_hct'].apply(lambda x: 1 if x == 'HIS' else 0)\n",
    "\n",
    "\n",
    "df['pediatric_and_arrhythmia'] = ((df['dri_score'] == 'N/A - pediatric') & (df['arrhythmia'] == 'Yes')).astype(int)\n",
    "\n",
    "df = categorize_hla_by_percantile(df)\n",
    "\n",
    "# Call the function, specifying the ID column to remove\n",
    "model, preprocessor, X_train, X_val, X_test, y_train, y_val, y_test = split_train_and_evaluate_with_sksurv(\n",
    "    df,\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    id_col='ID'  # Adjust this to match your actual ID column name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
